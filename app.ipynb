{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7725a4",
   "metadata": {},
   "source": [
    "**The Task:** Build a Python script that ingests raw text data, cleans it, and loads it into PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "361cfdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \"__main__\":\\n    df = fetch_multiprocess_dataset(\"google/civil_comments\", target_rows=1000000)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import worker_utils\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_superuser = os.getenv('DB_SUPERUSER')\n",
    "db_pass = os.getenv('DB_PASS')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "\n",
    "db_engine = create_engine(f'postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "def fetch_multiprocess_dataset(repo: str, target_rows: int = 500000, \n",
    "                        batch_size: int = 50000):\n",
    "    # Load the dataset stream (doesn't download everything at once)\n",
    "    dataset = load_dataset(repo, split=\"train\", streaming=True)\n",
    "    iter_ds = iter(dataset)\n",
    "\n",
    "    # List to store the \"Future\" results (placeholders for work being done)\n",
    "    futures = []\n",
    "    final_dfs = []\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        total_batches = (target_rows // batch_size)\n",
    "\n",
    "        for _ in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
    "            # Main process grabs the raw data (Network Bound)\n",
    "            batch = list(islice(iter_ds, batch_size))\n",
    "\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "            # Submit the data to a worker to convert (CPU Bound)\n",
    "            # This returns immediately with a \"Future\" object\n",
    "            future = executor.submit(worker_utils.process_batch, batch)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing...\"):\n",
    "            result_df = future.result()\n",
    "            final_dfs.append(result_df)\n",
    "\n",
    "    full_df = pd.concat(final_dfs, ignore_index=True)\n",
    "\n",
    "    # Trim exact amount (in case last batch went over)\n",
    "    full_df = full_df.head(target_rows)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "def read_csv_as_df(src, chunk_size: int = 10000):\n",
    "    dfs = {}\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for file in src:\n",
    "        subdir = re.split(r'/\\\\')\n",
    "        grouped[subdir].append(file)\n",
    "\n",
    "        for subdir_name, files in grouped.items():\n",
    "            table_chunks = []\n",
    "            \n",
    "            for file in files:\n",
    "                reader = pd.read_csv(file, index=False, chunksize=chunk_size)\n",
    "\n",
    "                for chunk in reader:\n",
    "                    table_chunks.append(chunk)\n",
    "        dfs = pd.concat(table_chunks, ignore_index=True)\n",
    "        dfs[subdir_name] = df\n",
    "\n",
    "        return dfs\n",
    "    \n",
    "def write_dfs_to_files_iter(dfs, base_path: str, file_format=\"csv\", attr_name=\"name\", **kwargs):\n",
    "    output_dir = f'{base_path}_{file_format}'\n",
    "    os.makedirs(f'{base_path}_{file_format}', exist_ok=True)\n",
    "\n",
    "    if isinstance(dfs, dict):\n",
    "        items = dfs.items() # Get table_name and values\n",
    "    else:\n",
    "        table_name = getattr(dfs, attr_name, attr_name)\n",
    "        items = [(table_name, dfs)]\n",
    "    \n",
    "    for table_name, df_or_reader in items:\n",
    "        # normalize: always make it a list of DataFrames\n",
    "        output_path = f'{output_dir}/{table_name}.{file_format}'\n",
    "\n",
    "        # Case 1 — direct DataFrame\n",
    "        if isinstance(df_or_reader, pd.DataFrame):\n",
    "            writer = getattr(df_or_reader, f\"to_{file_format}\", None)\n",
    "            if writer is None:\n",
    "                raise ValueError(f\"Unsupported format: {file_format}\")\n",
    "            writer(output_path, index=False, **kwargs)\n",
    "            continue\n",
    "\n",
    "        # Case 2 — chunked reader (TextFileReader)\n",
    "        is_first = True\n",
    "        writer_method = f\"to_{file_format}\"\n",
    "\n",
    "        for chunk in df_or_reader:\n",
    "            chunk.name = table_name\n",
    "            writer = getattr(chunk, writer_method, None)\n",
    "\n",
    "            if writer is None:\n",
    "                raise ValueError(f\"Unsupported file format for chunks: {file_format}\")\n",
    "            \n",
    "            if file_format == \"csv\":\n",
    "                writer(\n",
    "                    output_path,\n",
    "                    mode=\"w\" if is_first else \"a\",\n",
    "                    header=is_first,\n",
    "                    **kwargs\n",
    "                )\n",
    "            \n",
    "            elif file_format in (\"json\",):\n",
    "                # json doesn't support append properly → manual append simulation\n",
    "                writer(\n",
    "                    output_path,\n",
    "                    orient=\"records\",\n",
    "                    mode=\"w\" if is_first else \"a\",\n",
    "                    lines=True,\n",
    "                    **kwargs\n",
    "                )\n",
    "            elif file_format in (\"parquet\", \"feather\"):\n",
    "                # Parquet does NOT support append → we must collect in memory\n",
    "                # For large datasets, you'd use PyArrow dataset writer instead\n",
    "                if is_first:\n",
    "                    chunk.to_parquet(output_path, **kwargs)\n",
    "                else:\n",
    "                    chunk.to_parquet(\n",
    "                        output_path,\n",
    "                        append=True,  # works only if pyarrow supports it\n",
    "                        **kwargs\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                # Generic fallback: overwrite only on first chunk\n",
    "                writer(\n",
    "                    output_path,\n",
    "                    **kwargs\n",
    "                )\n",
    "            is_first = False\n",
    "\n",
    "\n",
    "def chunk_each_dataframe(dataframes: dict, chunk_size: int = 10000) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict of generators, one for each DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        dataframes: dict of DataFrames\n",
    "        chunk_size: number of rows per chunk\n",
    "        \n",
    "    Returns:\n",
    "        dict of generators with same keys\n",
    "    \"\"\"\n",
    "    chunked = {}\n",
    "    \n",
    "    for name, data in dataframes.items():\n",
    "        # Check if it's already a TextFileReader (chunked reader)\n",
    "        if isinstance(data, pd.io.parsers.TextFileReader):\n",
    "            # It's already chunked, just pass it through\n",
    "            chunked[name] = data\n",
    "        else:\n",
    "            # It's a DataFrame, create a chunking generator\n",
    "            def chunk_single_df(df, size=chunk_size):\n",
    "                for i in range(0, len(df), size):\n",
    "                    yield df.iloc[i:i + size]\n",
    "            \n",
    "            chunked[name] = chunk_single_df(data)\n",
    "    \n",
    "    return chunked\n",
    "\n",
    "def write_dfs_to_sql_iter(\n",
    "    dfs, \n",
    "    connection_string: str,\n",
    "    schema: str = None,\n",
    "    if_exists: str = 'replace',\n",
    "    chunksize: int = 1000,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Write DataFrames or chunked readers to SQL database.\n",
    "    \n",
    "    Args:\n",
    "        dfs: dict of DataFrames or TextFileReaders\n",
    "        connection_string: SQLAlchemy connection string\n",
    "        schema: database schema name (optional)\n",
    "        if_exists: 'fail', 'replace', or 'append'\n",
    "        chunksize: rows per write batch (for to_sql method)\n",
    "        **kwargs: additional arguments for to_sql\n",
    "    \"\"\"\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    if not isinstance(dfs, dict):\n",
    "        raise TypeError(\n",
    "            f\"dfs must be a dict, got {type(dfs).__name__}. \"\n",
    "            f\"Wrap single DataFrames like: {{'table_name': df}}\"\n",
    "        )\n",
    "    \n",
    "    # Create database engine\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    try:\n",
    "        for table_name, df_or_reader in dfs.items():\n",
    "            print(f\"Writing {table_name} to database...\")\n",
    "            \n",
    "            # Case 1 — Direct DataFrame\n",
    "            if isinstance(df_or_reader, pd.DataFrame):\n",
    "                df_or_reader.to_sql(\n",
    "                    name=table_name,\n",
    "                    con=engine,\n",
    "                    schema=schema,\n",
    "                    if_exists=if_exists,\n",
    "                    index=False,\n",
    "                    chunksize=chunksize,\n",
    "                    **kwargs\n",
    "                )\n",
    "                print(f\"✓ {table_name}: {len(df_or_reader)} rows written\")\n",
    "                continue\n",
    "            \n",
    "            # Case 2 — Chunked reader (generator)\n",
    "            is_first = True\n",
    "            total_rows = 0\n",
    "            \n",
    "            for chunk in df_or_reader:\n",
    "                if not isinstance(chunk, pd.DataFrame):\n",
    "                    raise ValueError(\n",
    "                        f\"Expected DataFrame chunk, got {type(chunk)}. \"\n",
    "                        f\"Reader may be exhausted or invalid.\"\n",
    "                    )\n",
    "                \n",
    "                # First chunk: replace or fail based on if_exists\n",
    "                # Subsequent chunks: always append\n",
    "                mode = if_exists if is_first else 'append'\n",
    "                \n",
    "                chunk.to_sql(\n",
    "                    name=table_name,\n",
    "                    con=engine,\n",
    "                    schema=schema,\n",
    "                    if_exists=mode,\n",
    "                    index=False,\n",
    "                    chunksize=chunksize,\n",
    "                    **kwargs\n",
    "                )\n",
    "                \n",
    "                total_rows += len(chunk)\n",
    "                is_first = False\n",
    "            \n",
    "            print(f\"✓ {table_name}: {total_rows} rows written\")\n",
    "    \n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "\"\"\"if __name__ == \"__main__\":\n",
    "    df = fetch_multiprocess_dataset(\"google/civil_comments\", target_rows=1000000)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55631d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 20/20 [05:08<00:00, 15.43s/it]\n",
      "Processing...: 100%|██████████| 20/20 [00:00<00:00, 46.33it/s]\n"
     ]
    }
   ],
   "source": [
    "df = fetch_multiprocess_dataset(\"google/civil_comments\", target_rows=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f6b28",
   "metadata": {},
   "source": [
    " **Workflow:**     1. Python script reads CSV.     2. Use Pandas to flag comments containing toxicity.     3. Separate data into two SQL tables: `clean_training_data` and `quarantined_toxic_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cd922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "\n",
    "toxicity_cols = [col for col in df.columns if col != 'text']\n",
    "toxicity_query_string = f' or '.join([f\"{col} > 0\" for col in toxicity_cols])\n",
    "safe_query_string = f' and '.join([f\"{col} <= 0\" for col in toxicity_cols])\n",
    "\n",
    "toxicity_mask = (df[toxicity_cols] > 0).any(axis=1)\n",
    "quarantine_mask = (df[toxicity_cols] <= 0).all(axis=1)\n",
    "\n",
    "quarantined_toxic_data = df[toxicity_mask].sort_values(\n",
    "    by=toxicity_cols,\n",
    "    ascending=False\n",
    ")\n",
    "clean_training_data = df[quarantine_mask].sort_values(\n",
    "    by=toxicity_cols, ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1a1589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>813887</th>\n",
       "      <td>Sorry, bucko, but I'll stay in Alaska - as I h...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618250</th>\n",
       "      <td>Dang folks, break out your happy lights or go ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791235</th>\n",
       "      <td>Nope, not a damn thing.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156196</th>\n",
       "      <td>As a primitive Native heathen, I do not share ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579515</th>\n",
       "      <td>Enough already.  Cut the head off this snake.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992498</th>\n",
       "      <td>http://www.secondenlightenment.org/ALL%20ABOAR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997545</th>\n",
       "      <td>Boeing could care less what Sajan and trudeau ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125976</th>\n",
       "      <td>Excellent, glad that you agree that a prospero...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318183</th>\n",
       "      <td>Allan can't read with his right eye tightly cl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261038</th>\n",
       "      <td>I'm sure once in a while they call him Buster ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306123 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxicity  \\\n",
       "813887  Sorry, bucko, but I'll stay in Alaska - as I h...       1.0   \n",
       "618250  Dang folks, break out your happy lights or go ...       1.0   \n",
       "791235                            Nope, not a damn thing.       1.0   \n",
       "156196  As a primitive Native heathen, I do not share ...       1.0   \n",
       "579515      Enough already.  Cut the head off this snake.       1.0   \n",
       "...                                                   ...       ...   \n",
       "992498  http://www.secondenlightenment.org/ALL%20ABOAR...       0.0   \n",
       "997545  Boeing could care less what Sajan and trudeau ...       0.0   \n",
       "125976  Excellent, glad that you agree that a prospero...       0.0   \n",
       "318183  Allan can't read with his right eye tightly cl...       0.0   \n",
       "261038  I'm sure once in a while they call him Buster ...       0.0   \n",
       "\n",
       "        severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "813887              0.4      0.8     0.0     0.9              0.0   \n",
       "618250              0.3      1.0     0.0     0.6              0.0   \n",
       "791235              0.3      1.0     0.0     0.6              0.0   \n",
       "156196              0.3      0.1     0.8     0.4              0.2   \n",
       "579515              0.3      0.0     1.0     0.4              0.0   \n",
       "...                 ...      ...     ...     ...              ...   \n",
       "992498              0.0      0.0     0.0     0.0              0.0   \n",
       "997545              0.0      0.0     0.0     0.0              0.0   \n",
       "125976              0.0      0.0     0.0     0.0              0.0   \n",
       "318183              0.0      0.0     0.0     0.0              0.0   \n",
       "261038              0.0      0.0     0.0     0.0              0.0   \n",
       "\n",
       "        sexual_explicit  \n",
       "813887         0.600000  \n",
       "618250         0.000000  \n",
       "791235         0.000000  \n",
       "156196         0.000000  \n",
       "579515         0.000000  \n",
       "...                 ...  \n",
       "992498         0.100000  \n",
       "997545         0.100000  \n",
       "125976         0.090909  \n",
       "318183         0.034483  \n",
       "261038         0.029412  \n",
       "\n",
       "[306123 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarantined_toxic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7adcc073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good...to protect the environment the EPA peop...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You know what I really don't care what people ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If port Townsend was smart it would lower its ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abigail, the rightful Queen of Hawaii.  Well a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>yeah ... and that's still a couple hundred not...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>There are at least 2 points that Goldberg igno...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>What is \"cofveve\"?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>Thank you GM for alerting the public about the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>He didn't come forward... he is a flight risk....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>693877 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxicity  \\\n",
       "0       Good...to protect the environment the EPA peop...       0.0   \n",
       "1                                             Interesting       0.0   \n",
       "2       You know what I really don't care what people ...       0.0   \n",
       "3       If port Townsend was smart it would lower its ...       0.0   \n",
       "4       Abigail, the rightful Queen of Hawaii.  Well a...       0.0   \n",
       "...                                                   ...       ...   \n",
       "999995  yeah ... and that's still a couple hundred not...       0.0   \n",
       "999996  There are at least 2 points that Goldberg igno...       0.0   \n",
       "999997                                 What is \"cofveve\"?       0.0   \n",
       "999998  Thank you GM for alerting the public about the...       0.0   \n",
       "999999  He didn't come forward... he is a flight risk....       0.0   \n",
       "\n",
       "        severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0                   0.0      0.0     0.0     0.0              0.0   \n",
       "1                   0.0      0.0     0.0     0.0              0.0   \n",
       "2                   0.0      0.0     0.0     0.0              0.0   \n",
       "3                   0.0      0.0     0.0     0.0              0.0   \n",
       "4                   0.0      0.0     0.0     0.0              0.0   \n",
       "...                 ...      ...     ...     ...              ...   \n",
       "999995              0.0      0.0     0.0     0.0              0.0   \n",
       "999996              0.0      0.0     0.0     0.0              0.0   \n",
       "999997              0.0      0.0     0.0     0.0              0.0   \n",
       "999998              0.0      0.0     0.0     0.0              0.0   \n",
       "999999              0.0      0.0     0.0     0.0              0.0   \n",
       "\n",
       "        sexual_explicit  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "...                 ...  \n",
       "999995              0.0  \n",
       "999996              0.0  \n",
       "999997              0.0  \n",
       "999998              0.0  \n",
       "999999              0.0  \n",
       "\n",
       "[693877 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e28deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_quarantine = chunk_each_dataframe({\"quarantined_toxic_data\": quarantined_toxic_data}, 50000)\n",
    "chunked_clean = chunk_each_dataframe({'clean_training_data': clean_training_data}, chunk_size=50000)\n",
    "\n",
    "write_dfs_to_files_iter(chunked_quarantine, './output', 'csv', attr_name='quarantined')\n",
    "write_dfs_to_files_iter(chunked_clean, './output', 'csv', attr_name='clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e417f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing clean_training_data to database...\n",
      "✓ clean_training_data: 693877 rows written\n",
      "Writing quarantined_toxic_data to database...\n",
      "✓ quarantined_toxic_data: 306123 rows written\n"
     ]
    }
   ],
   "source": [
    "db_url = os.getenv('DB_URL')\n",
    "\n",
    "chunked_quarantine = chunk_each_dataframe({\"quarantined_toxic_data\": quarantined_toxic_data}, 50000)\n",
    "chunked_clean = chunk_each_dataframe({'clean_training_data': clean_training_data}, chunk_size=50000)\n",
    "\n",
    "write_dfs_to_sql_iter(\n",
    "    chunked_clean,\n",
    "    db_url,\n",
    "    if_exists='replace',\n",
    "    chunksize=50000,\n",
    "    method='multi'\n",
    ")\n",
    "\n",
    "write_dfs_to_sql_iter(\n",
    "    chunked_quarantine,\n",
    "    db_url,\n",
    "    if_exists='replace',\n",
    "    chunksize=50000,\n",
    "    method='multi'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
